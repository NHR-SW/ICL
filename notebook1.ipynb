{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic in-context learning demonstration**\n",
    "\n",
    "In this first notebook, we will show a very simple introduction to in-context learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = #whatever key we get or what?\n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this hands-on session, we will use `gpt-3.5-turbo` and we will also use this helper function to easily test different in-context learning (ICL) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps\n",
    "\n",
    "As we have seen, LLM demonstrates an in-context learning (ICL) ability. In other words, they can learn from a few examples in the context. These examples are usually written in natural language templates. In this section, we will work with a couple of examples using sentiment analysis as our task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Review: A wonderful little production. Sentiment: Positive \\\n",
    "Review: Wow, what a bad film. Sentiment:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular task is a classical NLP task and not too complicated, so the performance is already quite good. For another kind of task, we could consider, for example, adding more demonstrations to our prompt and see if that affects the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Review: A wonderful little production. Sentiment: Positive \\\n",
    "Review: Bad plot, bad dialogue, bad acting. Sentiment: Negative \\\n",
    "Review: Such a boring movie. Sentiment: Negative \\\n",
    "Review:  A realistic portrayal of daily urban routines. Sentiment: Neutral \\\n",
    "Review: Wow, what a bad film. Sentiment:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were very basic templates, but there are several things we can play with in these prompts. For example, as we did, the amount of demonstrations, but also the formatting, the quality of demonstration, the ordering, etc. Taking the same example we just tested, we can reformate it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an NLP assistant for sentiment analysis in Chinese. Give your answer as \"positive\", \"negative\" or \"neutral\". \\\n",
    "Demonstration(s): A wonderful little production. What is the sentiment of this statement? Answer: Positive \\\n",
    "Test Input: Wow, what a bad film. What is the sentiment of this statement? Answer:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this hands-on session, we will do a small exploration of how these changes in our ICL prompts can affect the performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
