{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U  openai datasets -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903)\n",
    "\n",
    "> Abstract: We explore how generating a chain of thought—a series of **intermediate reasoning**\n",
    "steps—significantly **improves** the ability of large language models to perform\n",
    "complex reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cot.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Importing OpenAI and Writing Our Completion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from llm_config import llm_config, client\n",
    "\n",
    "def get_completion(messages, model=\"gpt-35-turbo\"):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "        )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Loading the MMLU Dataset\n",
    "\n",
    "In this mini tutorial we are working with MMLU dataset that is used to measure models mutli task accuracy. \n",
    "\n",
    "\n",
    "<img src=\"images/mmlu.png\" >\n",
    "\n",
    "\n",
    "the dataset is made of several task to check if the model have certain abilities \n",
    "\n",
    "```\n",
    "A complete list of tasks: ['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the MMLU dataset, specifically focusing on the subject of 'abstract algebra' for our exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/israela/Documents/repos/NHR/condor_analysis/.conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cais/mmlu\",\"abstract_algebra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the structure of a single question, along with its choices and answer pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.',\n",
       " 'subject': 'abstract_algebra',\n",
       " 'choices': ['0', '4', '2', '6'],\n",
       " 'answer': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = dataset['test'][0]\n",
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Improving Large Language Models with Chain-of-Thought Reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most advanced LLMs use chain-of-thought (CoT) reasoning for backend processing. To explore the model's responses without this feature, we will explicitly instruct it to provide answers without explanations. This approach will disable CoT reasoning and help us understand how the models perform without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"you are LLM capable of solving algebra questions given a question \\\n",
    "                                        answer without providing any explanation\n",
    "                                        \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {Q1['question']}\"}\n",
    "        ]\n",
    "response1 = get_completion(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 8\n"
     ]
    }
   ],
   "source": [
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the answer quality improves when we specifically request explanations, compared to the gold label we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"you are LLM capable of solving algebra questions given the following \\\n",
    "            question answer thinking step by step\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {Q1['question']}\"}\n",
    "        ]\n",
    "response2 = get_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Simplify sqrt(18) by factoring out the perfect square: sqrt(18) = sqrt(9 x 2) = 3sqrt(2)\n",
      "\n",
      "Step 2: Simplify the field extension by removing duplicates and reducing to a minimal set of generators: Q(sqrt(2), sqrt(3), sqrt(18)) = Q(sqrt(2), sqrt(3))\n",
      "\n",
      "Step 3: Determine the degree of the extension by finding the product of the degrees of the primitive extensions: [Q(sqrt(2), sqrt(3)) : Q] = [Q(sqrt(2)) : Q][Q(sqrt(3)) : Q]\n",
      "\n",
      "Step 4: Calculate the degrees of the primitive extensions using the fact that [Q(alpha) : Q] = degree of the minimal polynomial of alpha over Q.\n",
      "\n",
      "The minimal polynomial of sqrt(2) over Q is x^2 - 2, which has degree 2. Similarly, the minimal polynomial of sqrt(3) over Q is x^2 - 3, which also has degree 2.\n",
      "\n",
      "Therefore, [Q(sqrt(2), sqrt(3)) : Q] = [Q(sqrt(2)) : Q][Q(sqrt(3)) : Q] = 2 x 2 = 4.\n",
      "\n",
      "So the degree of the field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q is 4.\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Additional Chain-of-Thought Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs are easily confused by ambiguous questions, so providing examples can help clarify. Let's see how an LLM might fail on some examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"you are LLM capable answering questions in english give direct short response\"},\n",
    "            {\"role\": \"user\", \"content\": \"Take the last letter of the words in 'Lady Gaga' and concatenate them\"}\n",
    "        ]\n",
    "response3 = get_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaga'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples always help clarify how LLMs can be confused by certain types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"you are LLM capable answering questions in english\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "             For example if you are given 'Albert Einstein' the last letter of the first word is 't'\n",
    "             The last letter of the second word is 'n'\n",
    "             the output should be 'tn'\n",
    "             \n",
    "             Question: Take the last letter of the words in 'Lady Gaga' and concatenate them \"\"\"}\n",
    "        ]\n",
    "response4 = get_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The last letter of the first word is 'y' and the last letter of the second word is 'a'. Therefore, the output will be 'ya'.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are three t's in Teletubbies.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"you are LLM capable answering questions in english give direct short response\"},\n",
    "            {\"role\": \"user\", \"content\": \"how many t's is in Teletubbies\"}\n",
    "        ]\n",
    "response5 = get_completion(messages)\n",
    "response5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Chain-of-Thought and Detailed Output for Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when LLMs provide a wrong answer of a complicated quesion its hard to tell on which part that mistake was introuded at. \n",
    "Now we will see how we can get insight into the process of thinking and know where they are failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"you are LLM capable of solving algebra questions by writing a code,\\\n",
    "              write a python code to solve this problem\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {Q1['question']}\"}\n",
    "        ]\n",
    "response3 = get_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We know that the degree of a field extension is equal to the product of the degrees of the irreducible polynomials which generate the extension. \n",
      "\n",
      "First, we observe that:\n",
      "- sqrt(2) is a root of the irreducible polynomial x^2 - 2 over Q\n",
      "- sqrt(3) is a root of the irreducible polynomial x^2 -3 over Q\n",
      "- sqrt(18) is a root of the irreducible polynomial x^2 -18 = (x-sqrt(2)*sqrt(9))(x+sqrt(2)*sqrt(9)) over Q (notice that sqrt(9)=3 is already a term in this polynomial).\n",
      "\n",
      "Using the above observations, we can now determine the irreducible polynomial of the entire field extension:\n",
      "- We know Q(sqrt(2)) has degree 2 over Q\n",
      "- We know Q(sqrt(3), sqrt(2)) = Q(sqrt(3)+sqrt(2), sqrt(3)-sqrt(2)) has degree 2 over Q(sqrt(2)) since those two quantities are roots of x^2 - (2sqrt(2)+3) over Q(sqrt(2)) (we can obtain this by squaring (a+b*sqrt(2)) + (c+d*sqrt(2)), rationalizing denominators and equating coefficients).\n",
      "- We know Q(sqrt(2), sqrt(3), sqrt(18)) = Q(sqrt(2), sqrt(3)+sqrt(2), sqrt(3)-sqrt(2)) has degree 2 over Q(sqrt(3), sqrt(2)) since those two quantities are roots of x^2 - (6 + 2sqrt(2)) over Q(sqrt(3), sqrt(2)) (again, we can do similar calculations to the above).\n",
      "\n",
      "Thus, we have:\n",
      "degree(Q(sqrt(2), sqrt(3), sqrt(18)) over Q) = degree(Q(sqrt(2), sqrt(3)+sqrt(2), sqrt(3)-sqrt(2)) over Q(sqrt(3), sqrt(2))) * degree(Q(sqrt(3), sqrt(2)) over Q(sqrt(2))) * degree(Q(sqrt(2)) over Q)\n",
      "= 2 * 2 * 2\n",
      "= 8\n",
      "\n",
      "Therefore, the degree of the field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q is 8.\n"
     ]
    }
   ],
   "source": [
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deg3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDegree of Q(sqrt(2), sqrt(3), sqrt(18)) over Q:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdeg3\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deg3' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Degree of Q(sqrt(2), sqrt(3), sqrt(18)) over Q:\", deg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                   you are LLM capable of solving algebra questions by writing a code,\n",
    "                   write a python code to colve this problem.\n",
    "             \n",
    "                given a question \n",
    "             \n",
    "                # step 1:\n",
    "                     extract all the given variables and their value and required value \n",
    "             \n",
    "                # step 2:\n",
    "                     write a formula that you are going to use to solve the problem\n",
    "             \n",
    "                # step 3:\n",
    "                    write the steps to install all the necessery library\n",
    "             \n",
    "                # step 4:\n",
    "                    write a complete python function that solves a problem\n",
    "             \n",
    "                # step 5:\n",
    "                    Answer :  \n",
    "\n",
    "             \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {Q1['question']}\"}\n",
    "        ]\n",
    "response4 = get_completion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Step 1: \n",
      "The given variables and their values are:\n",
      "- base field: Q (rational numbers)\n",
      "- extension field: Q(sqrt(2), sqrt(3), sqrt(18))\n",
      "- square root of 2: sqrt(2)\n",
      "- square root of 3: sqrt(3)\n",
      "- required value: degree of field extension\n",
      "\n",
      "# Step 2: \n",
      "By using the multiplicativity property of field degree, we can find the degree of this field extension as the product of degrees of smaller sub-extensions:\n",
      "degree(Q(sqrt(2), sqrt(3), sqrt(18)) / Q) = degree(Q(sqrt(2), sqrt(3), sqrt(18)) / Q(sqrt(2), sqrt(3))) * degree(Q(sqrt(2), sqrt(3)) / Q(sqrt(2))) * degree(Q(sqrt(2)) / Q)\n",
      "\n",
      "Now, we need to find each of these degrees.\n",
      "\n",
      "# Step 3: \n",
      "No external libraries are required for this problem.\n",
      "\n",
      "# Step 4: \n",
      "Here is the python code to solve this problem:\n",
      "\n",
      "def degree_of_extension_field():\n",
      "    # sub-extension 1: Q(sqrt(2), sqrt(3)) / Q(sqrt(2))\n",
      "    degree1 = 2   # degree of Q(sqrt(2)) / Q is 2\n",
      "    degree2 = 2   # degree of Q(sqrt(2), sqrt(3)) / Q(sqrt(2)) is 2, as sqrt(3) is a root of x^2 - 3\n",
      "    degree3 = 1   # degree of Q(sqrt(2), sqrt(3), sqrt(18)) / Q(sqrt(2), sqrt(3)) is 1, as sqrt(18) is a root of x^2 - 18\n",
      "    degree_extension = degree1 * degree2 * degree3   # degree of full extension is product of sub-extension degrees\n",
      "    \n",
      "    return degree_extension\n",
      "\n",
      "# Step 5: \n",
      "The degree of the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q is 4. \n",
      "\n",
      "So, the answer is 4.\n"
     ]
    }
   ],
   "source": [
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_index_of_permutation(p):\n",
    "    n = len(p)\n",
    "    index = 0\n",
    "    for i in range(n-1):\n",
    "        sigma_i = sum(1 for j in range(i+1, n) if p[j] < p[i])\n",
    "        factorial = 1\n",
    "        for k in range(n-i, 0, -1):\n",
    "            factorial *= k\n",
    "        index += sigma_i * factorial\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Let's apply the same process to the random question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the index of <p> in S_5.',\n",
       " 'subject': 'abstract_algebra',\n",
       " 'choices': ['8', '2', '24', '120'],\n",
       " 'answer': 2}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = dataset['test'][1]\n",
    "Q1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
